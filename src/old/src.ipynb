{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9ba532f30a124189971b8e968e9995e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_604273e11bfb468cb657506b2fe6980b","IPY_MODEL_e201f408b1c44925804af3d04fda8a64","IPY_MODEL_f45cd51c822b43278304d7dae915a5ef"],"layout":"IPY_MODEL_d3c3fc3b7fe34f13a5772255a3d3eb02"}},"604273e11bfb468cb657506b2fe6980b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42b806697c4e48f5833a9f718f96ca40","placeholder":"​","style":"IPY_MODEL_8f013b51ebdd4221b78cc8c1acfc0351","value":"Map: 100%"}},"e201f408b1c44925804af3d04fda8a64":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_66cad0f8bcd0404ab48c47ab7680c068","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6170cdb9169c4d5eba605f429aeb3257","value":100}},"f45cd51c822b43278304d7dae915a5ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_572dea2d5d0843bca4510bcb98e6b684","placeholder":"​","style":"IPY_MODEL_d0f05aab1a7d4437928921177b5c0d28","value":" 100/100 [00:00&lt;00:00, 388.49 examples/s]"}},"d3c3fc3b7fe34f13a5772255a3d3eb02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42b806697c4e48f5833a9f718f96ca40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f013b51ebdd4221b78cc8c1acfc0351":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66cad0f8bcd0404ab48c47ab7680c068":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6170cdb9169c4d5eba605f429aeb3257":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"572dea2d5d0843bca4510bcb98e6b684":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0f05aab1a7d4437928921177b5c0d28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","\n","# Load the datasets\n","fact_check_post_mapping = pd.read_csv(\"trial_data_mapping.csv\")\n","fact_checks = pd.read_csv(\"trial_fact_checks.csv\")\n","posts = pd.read_csv(\"trial_posts.csv\")\n","\n","# Create a dataset with all possible pairs\n","posts_expanded = pd.merge(fact_check_post_mapping[['post_id']], posts[['post_id', 'text']], on='post_id')\n","# fact_checks_expanded = pd.merge(fact_check_post_mapping[['fact_check_id']], fact_checks[['fact_check_id', 'claim']], on='fact_check_id')\n","\n","# # Create all possible pairs between posts and fact-checks\n","# all_pairs = pd.merge(posts_expanded, fact_checks_expanded, how='cross')\n","\n","# # Assign labels based on the mapping file\n","# def assign_label(row):\n","#     if ((row['post_id'], row['fact_check_id']) in zip(fact_check_post_mapping['post_id'], fact_check_post_mapping['fact_check_id'])):\n","#         return 1\n","#     else:\n","#         return 0\n","\n","# all_pairs['label'] = all_pairs.apply(assign_label, axis=1)\n","\n","# # Drop rows with null values in 'text' or 'claim'\n","# all_pairs.dropna(subset=['text', 'claim'], inplace=True)\n","\n","# # Remove rows where 'text' or 'claim' are empty strings\n","# all_pairs = all_pairs[(all_pairs['text'].str.strip() != '') & (all_pairs['claim'].str.strip() != '')]\n","\n","# # Sample negative examples if the dataset is too large\n","# positive_examples = all_pairs[all_pairs['label'] == 1]\n","# negative_examples = all_pairs[all_pairs['label'] == 0]\n","\n","# # Optionally, you can downsample negative examples if they are too numerous\n","# negative_samples = negative_examples.sample(n=len(positive_examples), random_state=42)  # Balance dataset\n","\n","# # Combine positive and sampled negative examples\n","# data = pd.concat([positive_examples, negative_samples])\n","\n","# # Drop rows where 'text' or 'claim' is null or empty\n","# data.dropna(subset=['text', 'claim'], inplace=True)\n","# data = data[data['text'].str.strip() != '']  # Remove rows with empty strings\n","# data = data[data['claim'].str.strip() != '']  # Remove rows with empty strings\n","\n","# # Example view of the data\n","print(posts_expanded.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"19Cq8aVr_Hn0","outputId":"6c627b7a-db05-4161-d931-1cbed7bac9bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["     post_id                                               text  \\\n","0         30  ('★긴급] 일본 후쿠시마 원전에서 화재 발생...일본 정부, 주변 지역에 긴급대피...   \n","51        21  ('같은 김정은으로 보이지 않는데.. 지금은 세상에서 일어나는 일들이 마치 연극이나...   \n","102        7  ('<트랜스휴먼 : 유전자변형, 하이브리드 잡종, 인간개조>\\n\\n코로나백신 접종은...   \n","153       44  ('백신 접종을 하지 않은 사람들의 불법적인 봉쇄에 최대의 힘으로 저항하려고 길로 ...   \n","204       37  ('코로나 바이러스를 이용한 정부의 통제에 반발해서 일어난 루마니아 국민들. 국민들...   \n","\n","     fact_check_id                                              claim  label  \n","0               41  ('A massive fire broke out at the Fukushima nu...      1  \n","51              48  ('This photo shows Kim Jong-un in 2021', 'This...      1  \n","102             28  ('Covid-19 vaccines alter human DNA', 'Covid-1...      1  \n","153             12  ('This video shows an anti-lockdown rally in A...      1  \n","204             31  ('Photo shows anti-coronavirus restrictions pr...      1  \n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","from datasets import Dataset\n","\n","dataset = Dataset.from_pandas(data)\n","\n","# Load the mBERT tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","# Tokenization function for paired input (post + claim)\n","def tokenize_function(examples):\n","    return tokenizer(\n","        examples['text'],                 # Primary input (social media post)\n","        examples['claim'],                # Paired input (fact-check claim)\n","        truncation=True,                  # Truncate sequences longer than max length\n","        padding='max_length',             # Pad sequences to max length\n","        max_length=512                    # Max token length for BERT\n","    )\n","\n","# Tokenize dataset\n","dataset = dataset.map(tokenize_function, batched=True)\n","\n","# Example tokenized output\n","print(dataset[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["9ba532f30a124189971b8e968e9995e6","604273e11bfb468cb657506b2fe6980b","e201f408b1c44925804af3d04fda8a64","f45cd51c822b43278304d7dae915a5ef","d3c3fc3b7fe34f13a5772255a3d3eb02","42b806697c4e48f5833a9f718f96ca40","8f013b51ebdd4221b78cc8c1acfc0351","66cad0f8bcd0404ab48c47ab7680c068","6170cdb9169c4d5eba605f429aeb3257","572dea2d5d0843bca4510bcb98e6b684","d0f05aab1a7d4437928921177b5c0d28"]},"id":"iwwcdyj3_dbv","outputId":"7a2d425c-52e2-4df6-f80c-363ebaf2b7a3"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/100 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ba532f30a124189971b8e968e9995e6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'post_id': 30, 'text': \"('★긴급] 일본 후쿠시마 원전에서 화재 발생...일본 정부, 주변 지역에 긴급대피령 선포★\\\\n', '★Emergency] A fire breaks out at the Fukushima nuclear power plant in Japan...The Japanese government declares an emergency evacuation order in the surrounding area★\\\\n', [('kor', 1.0)])\", 'fact_check_id': 41, 'claim': \"('A massive fire broke out at the Fukushima nuclear power plant after the March 16 earthquake in eastern Japan', 'A massive fire broke out at the Fukushima nuclear power plant after the March 16 earthquake in eastern Japan', [('eng', 1.0)])\", 'label': 1, '__index_level_0__': 0, 'input_ids': [101, 113, 112, 1861, 70221, 37568, 166, 23130, 10003, 61156, 77901, 9612, 89326, 9993, 36210, 9323, 24017, 119, 119, 119, 23130, 9670, 14646, 117, 9689, 118985, 58939, 10530, 8933, 37568, 14423, 97146, 44220, 9428, 55530, 111744, 165, 182, 112, 117, 112, 1861, 11259, 12371, 11280, 11710, 166, 138, 13559, 68307, 10950, 10160, 10105, 17056, 87004, 20761, 13183, 16430, 10106, 11891, 119, 119, 119, 10117, 13847, 12047, 10104, 60582, 11234, 10151, 44461, 103730, 75545, 10822, 12990, 10106, 10105, 27027, 11168, 111744, 165, 182, 112, 117, 164, 113, 112, 33705, 112, 117, 122, 119, 121, 114, 166, 114, 102, 113, 112, 138, 35394, 13559, 30500, 10950, 10160, 10105, 17056, 87004, 20761, 13183, 16430, 10662, 10105, 11144, 10250, 63406, 10106, 18677, 11891, 112, 117, 112, 138, 35394, 13559, 30500, 10950, 10160, 10105, 17056, 87004, 20761, 13183, 16430, 10662, 10105, 11144, 10250, 63406, 10106, 18677, 11891, 112, 117, 164, 113, 112, 14716, 112, 117, 122, 119, 121, 114, 166, 114, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"]}]},{"cell_type":"code","source":["\n","from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n","\n","# Rename 'label' column to 'labels'\n","dataset = dataset.rename_column(\"label\", \"labels\")\n","\n","# Split dataset into train and test sets\n","train_test = dataset.train_test_split(test_size=0.2)\n","\n","# Load the mBERT model for sequence classification\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",               # Output directory\n","    evaluation_strategy=\"epoch\",          # Evaluate at the end of each epoch\n","    learning_rate=2e-5,                   # Learning rate\n","    per_device_train_batch_size=8,        # Batch size for training\n","    per_device_eval_batch_size=8,         # Batch size for evaluation\n","    num_train_epochs=3,                   # Number of epochs\n","    weight_decay=0.01,                    # Weight decay\n","    logging_dir=\"./logs\",                 # Directory for logging\n",")\n","\n","# Define the trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_test['train'],\n","    eval_dataset=train_test['test'],\n","    tokenizer=tokenizer\n",")\n","\n","# Train the model\n","trainer.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311},"id":"AvoG5i1GB7XE","outputId":"4c523ec6-3012-4a8c-b837-df1409426be3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [30/30 30:52, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.560809</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.524147</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.501934</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=30, training_loss=0.5112916628519694, metrics={'train_runtime': 1931.9782, 'train_samples_per_second': 0.124, 'train_steps_per_second': 0.016, 'total_flos': 63146653286400.0, 'train_loss': 0.5112916628519694, 'epoch': 3.0})"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["eval_results = trainer.evaluate()\n","print(eval_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"9DIXZCvYVuTq","outputId":"c9d0220b-7137-442c-83d9-42b2926a6e32"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:23]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.5019336938858032, 'eval_runtime': 42.7582, 'eval_samples_per_second': 0.468, 'eval_steps_per_second': 0.07, 'epoch': 3.0}\n"]}]},{"cell_type":"code","source":["import torch\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"oy-d91xEUYFY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare DataLoader for test dataset\n","test_loader = torch.utils.data.DataLoader(train_test['test'], batch_size=8)\n"],"metadata":{"id":"gfKm8LkUPpCq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to get predictions from the model\n","def get_predictions(test_dataset):\n","    model.eval()\n","    predictions = []\n","    true_labels = []\n","\n","    for batch in test_dataset:\n","        inputs = tokenizer(batch['text'], batch['claim'], padding=True, truncation=True, return_tensors=\"pt\")\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","        logits = outputs.logits\n","        predicted_labels = torch.argmax(logits, dim=1).tolist()\n","\n","        # Append predictions and true labels\n","        predictions.extend(predicted_labels)\n","        true_labels.extend(batch['labels'].tolist())\n","\n","    return predictions, true_labels"],"metadata":{"id":"PlddJqnMPqIW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get predictions\n","predictions, true_labels = get_predictions(test_loader)\n"],"metadata":{"id":"Xh5Qz2VzPvFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define evaluation metrics\n","def mean_reciprocal_rank(y_true, y_pred):\n","    mrr_total = 0.0\n","    for true, pred in zip(y_true, y_pred):\n","        for rank, doc_id in enumerate(pred):\n","            if doc_id in true:\n","                mrr_total += 1 / (rank + 1)\n","                break\n","    return mrr_total / len(y_true)\n","\n","def precision_at_k(y_true, y_pred, k=1):\n","    precision_total = 0.0\n","    for true, pred in zip(y_true, y_pred):\n","        relevant_count = len(set(true) & set(pred[:k]))\n","        precision_total += relevant_count / k\n","    return precision_total / len(y_true)\n"],"metadata":{"id":"svxJggMfPx69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_to_ranking(true_labels, predictions):\n","    # This assumes `predictions` are probabilities or scores, which are not directly usable in this form.\n","    # Here we treat `predictions` as if they are labels and create a list of lists for ranking.\n","    ranking_true = []\n","    ranking_pred = []\n","\n","    # Assume `predictions` are direct labels for simplicity\n","    for true, pred in zip(true_labels, predictions):\n","        ranking_true.append([true])  # true labels in ranking format\n","        ranking_pred.append([pred])  # predicted labels in ranking format\n","\n","    return ranking_true, ranking_pred"],"metadata":{"id":"IkGtf08oP3-E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ranking_true, ranking_pred = convert_to_ranking(true_labels, predictions)\n"],"metadata":{"id":"mRwfk6YCP88a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute metrics\n","mrr_score = mean_reciprocal_rank(ranking_true, ranking_pred)\n","precision_at_1 = precision_at_k(ranking_true, ranking_pred, k=1)\n","precision_at_5 = precision_at_k(ranking_true, ranking_pred, k=5)"],"metadata":{"id":"GWoS3d4nQAUX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Mean Reciprocal Rank (MRR): {mrr_score:.4f}\")\n","print(f\"Precision@1: {precision_at_1:.4f}\")\n","print(f\"Precision@5: {precision_at_5:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5yEyVNJjQJca","outputId":"621a4563-bb27-4c48-fba9-783a6c97e675"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Reciprocal Rank (MRR): 0.8500\n","Precision@1: 0.8500\n","Precision@5: 0.1700\n"]}]},{"cell_type":"markdown","source":["**Summary**\n","\n","Model is performing well in terms of ranking the most relevant fact-check at the top (as indicated by the high MRR and Precision@1).\n","\n","However, the model's performance drops for Precision@5, indicating that while the top-ranked result is usually correct, the quality of additional top results is not as high.\n"],"metadata":{"id":"JuXAPeeNWeBZ"}},{"cell_type":"code","source":["from sklearn.metrics import f1_score, precision_score, recall_score\n","import numpy as np\n","\n","# Ensure that `true_labels` and `predictions` are lists of integers\n","true_labels = np.array(true_labels)\n","predictions = np.array(predictions)\n","\n","# Calculate Precision, Recall, and F1 Score\n","precision = precision_score(true_labels, predictions)\n","recall = recall_score(true_labels, predictions)\n","f1 = f1_score(true_labels, predictions)\n","\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1 Score: {f1:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zLTIkpWyWcs1","outputId":"1be60405-e546-49cf-abaa-ae3b5745b6da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 0.8750\n","Recall: 0.7778\n","F1 Score: 0.8235\n"]}]},{"cell_type":"markdown","source":["Metrics Explanation\n","Precision: 0.8750\n","\n","Precision measures the proportion of true positive results out of all positive predictions made by the model.\n","\n","Interpretation: A Precision of 0.8750 means that when your model predicts a fact-check as relevant, it is correct 87.5% of the time. This indicates a high level of accuracy in the model's positive predictions.\n","Recall: 0.7778\n","\n","Recall measures the proportion of true positive results out of all actual positive instances in the dataset.\n","\n","Interpretation: A Recall of 0.7778 means that your model correctly identifies 77.78% of all relevant fact-checks in the dataset. This indicates that while the model is fairly good at finding relevant facts, there is still room for improvement in capturing all possible relevant cases.\n","F1 Score: 0.8235\n","\n","F1 Score is the harmonic mean of Precision and Recall. It provides a single metric that balances the trade-off between Precision and Recall.\n","\n","Interpretation: An F1 Score of 0.8235 indicates a strong balance between Precision and Recall. The model performs well in terms of both identifying relevant fact-checks and minimizing false positives.\n","\n","Summary\n","\n","Precision: Your model is very accurate when it identifies a fact-check as relevant, with a high proportion of correct positive predictions.\n","\n","Recall: The model identifies a good portion of all relevant fact-checks, but there are still some relevant cases it might be missing.\n","\n","F1 Score: The model achieves a good balance between Precision and Recall, reflecting overall strong performance."],"metadata":{"id":"RH82Q092XKmE"}},{"cell_type":"code","source":[],"metadata":{"id":"Z6UiY3xWWb3D"},"execution_count":null,"outputs":[]}]}